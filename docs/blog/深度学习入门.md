---
title: 深度学习入门
createTime: 2024/09/15 11:33:32
tags:
  - Deeping Learning
permalink: /article/xhr43d8p/
---
端到端机器学习（end to end machine learning）：从原始数据（输入）中获得目标结果（输出）

[ConvNetJS demo: Classify toy 2D data (stanford.edu)](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)

## 感知机

`感知机接收多个输入信号，输出一个信号`

> 1. 机器学习中人和机器的作用？
>
> 机器的任务：决定合适的参数值
>
> 人的任务：思考感知机的构造（模型），并把训练数据交给计算机
>
> 2. 理论上用感知机制作计算机？
>
> 分阶段地制作所需的零件（模块），即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元(ALU)，然后实现CPU。

### 公式介绍

- 阈值（$\theta$）：输入信号被送往神经元时，被分别乘以固定的权重（$w_{1}x_{1}\text{、}w_{2}x_{2}$），当总和超过阈值时输出1。
- 权重（$w$）：越大，对应该权重的信号的重要性越高。

$$
y=\left\{\begin{array}{ll}0&(w_1x_1+w_2x_2\leqslant\theta)\\1&(w_1x_1+w_2x_2>\theta)\end{array}\right.
$$
改进：将$\theta$转换为$-b$

- 权重（$w$）：控制输入信号的重要性
- 偏置（$b$）：调整神经元被激活的容易程度

$$
y=\left\{\begin{array}{ll}0&(b+w_1x_1+w_2x_2\leqslant0)\\1&(b+w_1x_1+w_2x_2>0)\end{array}\right.
$$

### 逻辑电路与感知机

#### 单层感知机-与或非

`又称朴素感知机`

- 与门（AND gate）：仅在两个输入均为1时输出1，其他时候则输出0
  参数举例：$(w_{1},w_{2},\theta)=(0.5,0.5,0.8)$
  真值表如下：

    | $x_{1}$ | $x_{2}$ | $y$  |
    | ------- | ------- | ---- |
    | 0       | 0       | 0    |
    | 1       | 0       | 0    |
    | 0       | 1       | 0    |
    | 1       | 1       | 1    |

```py
# 简易实现
def AND(x1,x2):
    w1, w2, theta=0.5,0.5,0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```

```py
# numpy实现
def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])	# 权重
    b = -0.7	# 偏置
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

- 与非门（NAND gate）：仅当$x_{1}$和$x_{2}$同为1时输出0，其他时候输出1
  参数举例：$(w_{1},w_{2},\theta)=(-0.5,-0.5,-0.7)$

    | $x_{1}$ | $x_{2}$ | $y$  |
    | ------- | ------- | ---- |
    | 0       | 0       | 1    |
    | 1       | 0       | 1    |
    | 0       | 1       | 1    |
    | 1       | 1       | 0    |

```py
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5]) # 仅权重和偏置与AND不同！
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

- 或门（OR gate）：只要有一个输入信号是1，输出就为1

    | $x_{1}$ | $x_{2}$ | $y$  |
    | ------- | ------- | ---- |
    | 0       | 0       | 0    |
    | 1       | 0       | 1    |
    | 0       | 1       | 1    |
    | 1       | 1       | 1    |

```py
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5]) # 仅权重和偏置与AND不同！
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### 多层感知机-异或

`神经网络的初级阶段`

- 异或门（也叫异或逻辑电路）：仅当$x_{1}$或$x_{2}$中的一方为1时，才会输出1

    | $x_{1}$ | $x_{2}$ | $y$  |
    | ------- | ------- | ---- |
    | 0       | 0       | 0    |
    | 1       | 0       | 1    |
    | 0       | 1       | 1    |
    | 1       | 1       | 0    |


使用双层感知机实现异或门：

| $x_{1}$ | $x_{2}$ | $s_{1}$ | $s_{2}$ | $y$  |
| ------- | ------- | ------- | ------- | ---- |
| 0       | 0       | 1       | 0       | 0    |
| 1       | 0       | 1       | 1       | 1    |
| 0       | 1       | 1       | 1       | 1    |
| 1       | 1       | 0       | 1       | 0    |

![image-20240911094250880](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240911094250880.png)

```py
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```



## 全连接的推理

`优势：自动从数据中学习到合适的权重参数`

![image-20240911094848754](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240911094848754.png)

### 感知机 -> 全连接（NN）

感知机函数拆分为算法和激活函数：
$$
a=b+w_1x_1+w_2x_2
$$

$$
y=h(a)
$$

- 单层感知机（朴素感知机）使用阶跃函数作为激活函数
- 多层感知机使用平滑的激活函数，如sigmoid函数

### ==激活函数==

`使神经网络加入非线性特点`

> 问：激活函数为什么必须使用非线性函数？
>
> 答：线性函数无法发挥叠加层的优势，如$y(x)=h(h(h(x)))=c^3x$

#### 阶跃函数

`> 阈值的元素被转换为True；`
`<= 阈值的元素被转换为False。`

![output](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/output.png)

```py
def step_function(x):
    # 此处的阈值为0
    y = x > 0
    return y.astype(np.int)
```

#### sigmoid

> 优点：
>
> 1. 平滑
> 2. 将数值转化为概率

$$
h(x)=\frac1{1+e^{(-x)}}
$$

关于(0,0.5)对称的S型曲线

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/output-1726142869224-2.png" style="zoom: 67%;" />

```py
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

#### tanh

关于原点(0,0)对称的双S型曲线函数

#### ReLU

$$
h(x)=\left\{\begin{array}{ll}x&(x>0)\\0&(x\leqslant0)\end{array}\right.
$$

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/output-1726144156894-4.png" style="zoom:67%;" />

```py
def relu(x):
    return np.maximum(0, x)
```

#### 恒等函数

$$
y=x
$$

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/output-1726145086461-6.png" style="zoom:67%;" />

```py
def identity_function(x):	# 恒等函数
    return x
```

#### softmax

> 优势：
>
> 1. 元素间大小关系不变（指数函数单调递增）
> 2. 数值转化为概率（实际分类问题只需要求最高数值，所以输出层的softmax会被省略）
> 3. 输出总和为1【归一化】

$$
y_k=\frac{e^{a_k}}{\sum_{i=1}^n e^{a_i}}
$$

```py
def softmax_origin(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

解决指数的溢出问题？指数减去常数C（输入信号的最大值）
$$
\begin{aligned}y_{k}=\frac{e^{a_{k}}}{\sum_{i=1}^{n}e^{a_{i}}}&=\frac{\mathrm{C}e^{a_{k}}}{\mathrm{C}\sum_{i=1}^{n}e^{a_{i}}}\\&=\frac{e^{(a_{k}+\log C)}}{\sum_{i=1}^{n}e^{(a_{i}+\log C)}}\\&=\frac{e^{(a_{k}+\mathrm{C}^{\prime})}}{\sum_{i=1}^{n}e^{(a_{i}+\mathrm{C}^{\prime})}}\end{aligned}
$$

```py
def softmax_improve(a):
    c = np.max (a)
    exp_a = np.exp(a-c)	# 防止溢出
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

### 神经网络图

![image-20240912203525963](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240912203525963.png)

参数解析：

- 右上角的（1）：表示权重和神经元的层号
- 右下角：后一层神经元到索引号、前一层的索引号
- 为什么偏置右下角的索引号只有一个？前一层的偏置神经元（神经元“1”）只有一个

![image-20240913080442865](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240913080442865.png)

### 手写数字识别：推理

> 前向传播：测试过程中神经网络的推理过程

> 预处理：对输入数据进行转换的行为
>
> - 如何对数据进行转换？均值、标准差，移动数据，数据整体以0为中心分布，正规化，数据整体的分布形状均匀化（数据白化）

> 正规化：将数据限定到某个范围内的处理。如：将图像各个像素值除以255，数据值范围为[0.0~1.0]

> 批处理：如一次性打包100张图作为输入数据
>
> - 优势：缩短处理时间（大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化）
>
> ![image-20240915170247814](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240915170247814.png)

```py
import sys, os
sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定
import numpy as np
import pickle
from dataset.mnist import load_mnist
from common.functions import sigmoid, softmax
```

```py
def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

 
def init_network():
    with open("E:\Code\Python\deep_deeping\【源代码】深度学习入门：基于Python的理论与实现\【源代码】深度学习入门：基于Python的理论与实现\ch03\sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):    # 前向传播进行预测
    # 将784特征 -> 50 -> 100 -> 10 -> 10个概率，挑选出最大的概率的索引即为预测值
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1     # (784) (784*50) (50,)
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2    # (10000*50) (50*100) (100,)
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y
```

`数组形状变化图`

![image-20240915170218952](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240915170218952.png)

```py
img_test, label_test = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(img_test)):
    y = predict(network, img_test[i])
    p= np.argmax(y) # 获取概率最高的元素的索引
    if p == label_test[i]:
        accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(img_test)))
```

## 全连接的学习

`从训练数据中，以损失函数为基准，找出使损失函数值最小的权重参数`

为了找出尽可能小的损失函数的值，本章介绍函数斜率的梯度法

### 历代求解问题方法

举例（识别数字5）【白色框为人为介入】：

![image-20240915173143225](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240915173143225.png)

- `人为设计算法`
- `计算机视觉一般方法`：提取特征量（SIFT、SURF和HOG等），使用特征量将图像数据转换为向量，对向量使用机器学习中的SVM、KNN等分类器进行学习
- `深度学习方法`：机器自己提取特征自己学习

### ==损失函数==

> 损失函数的作用：
>
> 1. 寻找最优权重参数
> 2. 当前神经网络对监督数据在多大程度上不拟合

#### 均方误差

`mean squared error`
$$
E=\frac{1}{2}\sum_{k}(y_{k}-t_{k})^{2}
$$

- $y_{k}$：神经网络的输出
- $t_{k}$：监督数据
- $k$：数据的维数

```py
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

#### 交叉熵误差

`cross entropy error，常用于分类问题`

$$
E=-\sum_ty_t\log \hat{y_t}
$$

- $y_{t}$：真实值
- $\hat{y_{t}}$：预测值

```py
# 多分类【如果输出层只有两个神经元，则公式与二分类一致】
def cross_entropy_error(y, t):
    # 为什么要加入delta？防止np.log(0)的出现导致溢出
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

$$
E=-(t\log y+(1-t)\log(1-y))
$$

```py
# 二分类
```



#### 如何求平均损失？

`通过正规化求平均损失函数`
$$
E=\frac{1}{n}\sum_n(\text{损失函数})
$$

### mini-batch

#### 样本随机抽取实现

`像是抽样调查`

```py
train_size = img_train.shape[0]		# 训练样本数
batch_size = 10		# 批次大小
batch_mask = np.random.choice(train_size, batch_size)
# 此函数从总训练样本中随机抽取批次大小个标记数值
img_batch = img_train[batch_mask]	# 根据标记数值从训练数据求训练样本
label_batch = label_train[batch_mask]	# 根据标记数值从训练数据求训练样本的标签
```

### 交叉熵+平均损失+不同编码

当监督数据为one-hot编码形式时：

```python
t * np.log(y)
```

```py
def cross_entropy_error(y, t):
    # y：神经网络的输出。(batch_size,)
    # t：监督数据。如：[1,0,0,1,0]
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

当监督数据为标签形式时：

```py
np.log(y[np.arange (batch_size), t] )
```

```py
def cross_entropy_error(y, t):
    # y：神经网络的输出。(batch_size,)
    # t：监督数据。如：[2,7,0,9,4]
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

####  梯度如何指引方向？

`损失函数的导数就是梯度，以梯度为指引，更新参数的值`

- 导数值 < 0 时：可以使权重参数向正方向改变来减少损失函数的值
- 导数值 > 0 时：可以使权重参数向负方向改变来减少损失函数的值
- 导数值 = 0 时：无论权重参数向哪个方向变化，损失函数都不会改变，此时该权重参数的更新会停在此处

为什么不使用识别精度作为指标？精度是一个数字，微调对权重参数影响不大，绝大多数地方的导数都会变为0，进而导致参数无法更新

### 梯度

`损失函数的导数`

> 机器学习的最终目标：获取泛化能力
>
> 1. 使用训练数据（监督数据）进行学习，提升泛化能力
> 2. 使用测试数据评价泛化能力
>
> 欠拟合和过拟合：
>
> - 欠拟合：模型复杂度过低、特征量过少，无法在训练集获得足够低的误差
> - 过拟合：模型复杂度高，训练集表现好，测试机表现差

#### 数值微分

`利用微小的差分求导数的过程`

> 为什么不使用公式法直接求导？
>
> 无法满足所有求导，函数复杂则难直接求导

- 前向差分

$$
\frac{\mathrm{d}f(x)}{\mathrm{d}x}=\lim_{h\to0}\frac{f(x+h)-f(x)}h
$$

- 中心差分

$$
\frac{\mathrm{d}f(x)}{\mathrm{d}x}=\lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h}
$$

```py
# 代码实现
# 1.防止h无限小无法计算，设置为1e-4
# 2.使用中心差分法减小误差
def numerical_diff(f, x):
    h = 1e-4	# 0.0001
    return (f(x+h) - f(x-h)) / (2*h)
```

求偏导：
$$
f(x_0,x_1)=x_0^2+x_1^2
$$
![image-20240917104359888](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240917104359888.png)

```py
def function_tmp1(x0):
    # 将x1固定为4
    return x0*x0 + 4.0**2.0
def function_tmp2(x1):
    # 将x0固定为3
    return 3.0**2.0 + x1*x1

# 在(3,4)处求x0偏导
numerical_diff(function_tmp1, 3.0)
# 在(3,4)处求x0偏导
numerical_diff(function_tmp2, 4.0)
```

#### 了解梯度

> 为什么使用梯度来更新参数？
>
> 损失函数为凹函数，存在极小值（极点）和最小值，通过求导的方法可以求得使损失函数值最小的参数

$f(x_0,x_1)=x_0^2+x_1^2$的梯度图：

![](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/Figure_1.png)

解析：损失函数为凹函数，为了损失函数变小（即收敛），通过向箭头指向位置更新参数x0和x1（即损失函数的导数趋于0的位置）。

```py
"""
计算导数：数值微分(数字) -> 梯度(数组)：
"""
def numerical_gradient(f, x):
    # f：损失函数
    # x：数组
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # 初始化梯度数组

    for idx in range(x.size):
        # 对数组中的每个值计算损失函数的导数
        tmp_val = x[idx]
        # f(x+h)的计算
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h)的计算
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 还原数组

    return grad
```

#### 梯度法

`损失函数的参数从当前位置沿着导数小于0的方向前进一段距离，再次求导数，再次前进，逐渐减小损失值，最终求得最小值/极小值的过程`

> 名词解释：
>
> - 梯度下降法：寻找最小值的梯度法
>
> - 梯度上升法：寻找最大值的梯度法
>
>
> ==为了求最值/极值，使用哪个梯度法并不重要（切换损失函数前的符号即可切换方法），最终都会走向收敛==

梯度法公式：
$$
x_{0}=x_{0}-\eta\frac{\partial f}{\partial x_{0}}\\x_{1}=x_{1}-\eta\frac{\partial f}{\partial x_{1}}
$$

- $\eta$：学习率（决定在一次学习中，应该学习多少，多大程度上更新参数）
  ==学习率过大：发散成一个很大的值；==
  ==学习率过小：基本上没怎么更新就结束==
  ==一般会一边改变学习率的值，一边确认学习是否正确进行了==

```py
# 梯度下降法
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x
```

#### 梯度法实现

```py
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 初始化【标准正态分布（高斯分布）】

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        # x：输入数据
        # t：正确解
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

        return loss
```

```py
def f(W):
    return net.loss(x,t)
```

```py
t = np.array([0,0,1])	# 正确解
net.loss(x,t)	# 算损失
dW = numerical_gradient(f,net.W)	# 求梯度
dW
```

### ==构建初级神经网络==

> epoch：训练数据被使用一次epoch+1
>
> mini-batch：随机选取训练数据（使用此方法训练成为随机梯度下降法SGD）

神经网络的学习步骤：

1. 选取训练数据：batch
2. 计算梯度：损失函数值减小最多的方向
3. 更新参数：权重参数沿梯度方向进行微小更新
4. 重复1-3

- batch选取：min-batch、等

#### 基本类

```py
import sys, os
sys.path.append(os.pardir)
from common.functions import *
from common.gradient import numerical_gradient

# 双层神经网络【x：输入数据；t：监督数据】
class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size,
                 weight_init_std=0.01):
        # 初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)

        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    # 进行预测
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        return y

    # 计算损失函数的值
    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy_error(y, t)
    
    # 计算识别精度
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    # 计算权重参数的梯度
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        # 初始化梯度
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads
```
#### 随机小批量

```py
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)

train_loss_list = []

# 超参数
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 获取mini-batch(随机选择)
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 计算梯度
    # grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch) # 高速版!

    # 更新参数
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 记录学习过程
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```
#### 记录精度（epoch）

```py
import numpy as np
from dataset.mnist import load_mnist
# from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) =  load_mnist(normalize=True, one_hot_label = True)

train_loss_list = []
train_acc_list = []
test_acc_list = []
# 平均每个epoch的重复次数
iter_per_epoch = max(train_size / batch_size, 1)

# 超参数
iters_num = 10000
batch_size = 100
learning_rate = 0.1

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 获取mini-batch
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 计算梯度
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) # 高速版!

    # 更新参数
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    # 计算每个epoch的识别精度
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
```

## 误差反向传播

`数值微分速度太慢，误差反向传播法可以极大的提升计算速度`

### 计算图

`计算图将计算过程用图形表示出来`

> 计算图的优点：
>
> 1. 通过反向传播高效计算导数
> 2. 局部计算（通过局部计算使各个节点致力于简单的计算，从而简化问题）
> 3. 中间计算结果被保存并共享利于高效计算多个导数

计算图举例：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100日元，橘子每个150日元。消费税是10%，请计算支付金额？

![image-20240918092122566](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918092122566.png)

- 正向传播：从左向右进行计算
- 反向传播：从右向左进行计算

### 链式法则

#### 计算图的反向传播

==反向传播的计算顺序==：将节点的输入信号乘以节点的局部倒数（偏导数），然后再传递给下一个节点

![image-20240918140438143](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918140438143.png)

#### 计算图的链式法则

公式：
$$
\left\{ 
\begin{aligned}
z &= t^2 \\
t &= x + y
\end{aligned}
\right.
$$
求导：
$$
\frac{\partial z}{\partial x}=\frac{\partial z}{\partial t}\frac{\partial t}{\partial x}=2t·1=2(x+y)
$$
反向传播计算图：

![image-20240918143651688](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918143651688.png)

`加法的反向传播`：将上游的值传给下游，并不需要正向传播的输入信号

`乘法的反向传播`：需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号

苹果和橘子问题反向传播计算图：

![image-20240918151307894](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918151307894.png)

### 简单层的实现

#### 加法层

`加法层的反向传播：乘1`

```py
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y
        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```

#### 乘法层

`乘法层的反向传播：乘“翻转的x和y”`

```py
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y # 翻转x和y
        dy = dout * self.x

        return dx, dy
```

#### 举例：苹果和橘子问题

```py
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

# layer
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num) #(1)
orange_price = mul_orange_layer.forward(orange, orange_num) #(2)
all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)
price = mul_tax_layer.forward(all_price, tax) #(4)

# backward
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice) #(4)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)
dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)
dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)

print(price) # 715
print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650
```

### 激活函数-反向传播

`函数、计算图、导数`

#### ReLU

$$
y=\begin{cases}x&(x>0)\\0&(x\leqslant0)\end{cases}
$$

![image-20240918203311829](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918203311829.png)
$$
\frac{\partial y}{\partial x}=\begin{cases}1&(x>0)\\0&(x\leqslant0)\end{cases}
$$

```py
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        # (x<=0)：此表达式为判断x中元素是否<0，若<0则为True赋值给mask
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        # 反向传播中会使用正向传播时保存的mask，将从上游传来的dout的mask中的元素为True的地方设为 0
        dout[self.mask] = 0
        dx = dout

        return dx
```

#### Sigmoid

$$
y=\frac{1}{1+\exp(-x)}
$$

![image-20240918203204507](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918203204507.png)
$$
\frac{\partial y}{\partial x}=\frac{\partial L}{\partial y}y(1-y)
$$

```py
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        # 正向传播将输出保存在out中
        out = 1 / (1 + np.exp(-x))
        self.out = out

        return out

    def backward(self, dout):
        # 反向传播使用out进行计算
        dx = dout * (1.0 - self.out) * self.out

        return dx
```



### Affine层

> 仿射变换：在几何中包括一次线性变换和一次平移，即加权和运算、加偏置运算

单数据Affine层计算图：

![image-20240918204945600](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918204945600.png)

批版本Affine层计算图：

![image-20240918210329672](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240918210329672.png)

偏置在正向传播中使用广播机制加到$x*w$中

反向传播中各个数据的反向传播值要汇总为偏置的元素

```py
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        return dx
```

### Softmax-with-loss层

`softmax将数值正规化转化为概率`

推理不需要softmax：推理求最大项

学习需要softmax：作为每一层的激活函数

```py
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None # 损失
        self.y = None    # softmax的输出
        self.t = None    # 监督数据(one-hot vector)

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        # 交叉熵损失函数
        self.loss = cross_entropy_error(self.y, self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        # 反向传播时，将要传播的值除以批的大小(batch_size)后，传递给前面的层的是单个数据的误差
        dx = (self.y - self.t) / batch_size

        return dx
```

### 误差反向传播实现

#### 函数

```py
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict

class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size,
                 weight_init_std=0.01):
        # 初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        # 层设计
        # 运行：以正确的顺序连接各层【正向传播】，再按顺序（或者逆序）调用各层【反向传播】
        self.layers = OrderedDict() # 将神经网络层保存在OrderedDict中
        # 输入、隐藏、输出层
        self.layers['Affine1'] = \
            Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()

        self.layers['Affine2'] = \
            Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()

    # 进行推理
    def predict(self, x):
        # x:图像数据
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    # 计算损失函数的值
    def loss(self, x, t):
        # x:输入数据, t:监督数据
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    # 计算识别精度
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # 通过数值微分计算关于权重参数的梯度
    def numerical_gradient(self, x, t):
        # x:输入数据, t:监督数据
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

    # 通过误差反向传播法计算关于权重参数的梯度
    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 设定
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db

        return grads
```

```py
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# 读入数据
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 通过误差反向传播法求梯度
    grad = network.gradient(x_batch, t_batch)

    # 更新
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)
```

#### 梯度确认

> 为什么要确认梯度？
>
> 误差反向传播法高效，但较为复杂。使用数值微分法的结果和误差反向传播法的结果进行比较，以确认误差反向传播法是否正确

```py
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# 读入数据
(x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, one_
hot_label = True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# 求各个权重的绝对误差的平均值
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
    print(key + ":" + str(diff))
```

## 训练技巧

### 权重参数的初始化

`设定合适的权重初始值，各层的激活值分布会有适当的广度，从而顺利学习`

`问`：权重初始值设为0可以吗？

`答`：如果首先第二层的权重就会被相同更新（拥有对称值），为防止“权重均一化”，必须随机生成初始值

```PY
w = 0.01 * np.random.randn(D,H)
```

#### 对激活函数的影响

> - 梯度消失：激活函数的导数逐渐趋于0（如：使用sigmoid函数作为激活函数）
> - 表现力受限：激活函数输出几乎相同的值（如：标准差为0.01）
>
> 为了防止这两种情况，各层激活函数值分布都要求有适当的广度：各层间传递多样性数据有利于高效学习

#### Xavier初始值-sigmoid、tanh

前一层的节点数为n，初始值使用标准差为$\frac{1}{\sqrt{n}}$的高斯分布

![image-20240920133056982](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920133056982.png)

使用tanh函数代替sigmoid函数，会改善歪斜的问题（关于原点对称）

#### He初始值-relu

前一层的节点数为n，初始值使用标准差为$\frac{2}{\sqrt{n}}$的高斯分布

![image-20240920133638006](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920133638006.png)

### 权重参数的更新方法

> 最优化（optimization）：寻找最优参数

#### SGD

`随机梯度下降法（stochastic gradient descent/SGD）：使用参数的梯度，沿梯度方向更新参数，并重复步骤，从而逐渐靠近最优参数`

> SGD低效的根本原因：梯度的方向并没有指向最小值的方向

$$
W\leftarrow W-\eta\frac{\partial L}{\partial W}
$$

- $\eta$表示学习率

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920084024400.png" alt="image-20240920084024400" style="zoom: 67%;" />

```py
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr    # 学习率

    def update(self, params, grads):    # 更新权重参数
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

#### Momentum

`根据历史梯度更新方向稳定当前轮次梯度更新方向`

> 模拟运动的惯性，在每一步更新中，考虑当前的梯度信息和之前步骤的动量信息，来平滑梯度的波动并加速收敛

$$
v\leftarrow\alpha v-\eta\frac{\partial L}{\partial W}\\W\leftarrow W+v
$$

- a：动量衰减系数
- v：当前动量

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920101033514.png" alt="image-20240920101033514" style="zoom:67%;" />

```py
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None	# 初始化时，v为空

    def update(self, params, grads):
        if self.v is None:
            # 第一次调用update时：赋值给v参数相同数据（除了值为0）
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]
```

#### AdaGrad

> 学习率衰减（learning rate decay）：随着学习的进行，按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小
>
> - 学习率衰减为0：使用RMSProp方法，逐渐遗忘过去的梯度【指数移动平均：呈指数函数式地减小过去的梯度的尺度】

$$
h\leftarrow h+\frac{\partial L}{\partial W}\odot\frac{\partial L}{\partial W}\\W\leftarrow W-\eta\frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}
$$

- $\odot$：对应矩阵元素乘积
- h：以前的所有梯度值的平方和

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920125130770.png" alt="image-20240920125130770" style="zoom:67%;" />

```py
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            # 加微小值1e-7防止溢出
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

#### Adam

> 1. 融合了Monmentum和AdaGrad方法
> 2. 超参数的“偏置校正”

Adam的参数：

- 学习率：$\alpha$
- momentum系数：$\beta_{1}$
- 二次momentum系数：$\beta_{2}$

#### 结果的影响因素

1. 根据要解决的问题而不同
2. 学习率等超参数、神经网络的结构（几层深等）的不同而不同
3. 一般来说，除了SGD，其他三种方法学习更快，识别精度更高

### 正则化惩罚

`为了抑制过拟合`

#### 过拟合

> 过拟合的原因：
>
> - 模型拥有大量参数、表现力强
> - 训练数据少

#### 权值衰减

`对大的权重进行惩罚来抑制过拟合`

**举例：**

为损失函数加上权重的$L_{2}$范数->抑制权重变大

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240926131809749.png" alt="image-20240926131809749" style="zoom:50%;" />

如：$\frac{1}{2}\lambda W^{2}$


- $\lambda$：控制正则化强度的超参数。（越大，对大的权重施加的惩罚越重）
- $\frac{1}{2}$：将$\frac{1}{2}\lambda W^{2}$的求导结果变为$\lambda W$

**权值衰减方法：**

1. 为损失函数加上$\frac{1}{2}\lambda W^{2}$
2. 求权重梯度中，为之前的误差反向传播法的结果加上正则化项的导数$\lambda W$

### batch normalization（批归一化）

`调整激活函数的值的分布使其拥有适当的广度`

以进行学习时的mini-batch为单位，按mini-batch进行正规化（进行使数据分布的均值为0、方差为1的正规化）

batch norm的优点：

- 使学习快速进行（增大学习率）
- 不那么依赖初始值
- 抑制过拟合（降低Dropout等的必要性）

![image-20240920135741507](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920135741507.png)

步骤：

1. 将mini-batch的输入数据转换为均值为0、方差为1的数据（减小数据分布的偏向）

$$
\begin{aligned}&\mu_{B}\leftarrow\frac{1}{m}\sum_{i=1}^{m}x_{i}\\&\sigma_{B}^{2}\leftarrow\frac{1}{m}\sum_{i=1}^{m}(x_{i}-\mu_{B})^{2}\\&\hat{x}_{i}\leftarrow\frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\varepsilon}}\end{aligned}
$$

2. 对正规化后的数据进行缩放和平移变换（一开始$\gamma$=1,$\beta$=0，再通过学习调整到合适的值）

$$
y_i\leftarrow\gamma\hat{x}_i+\beta
$$

![image-20240920144952611](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240920144952611.png)

### Dropout

`随机删除神经元`

> 适用于复杂的网络模型（只使用权值衰减难以应对时）
>
> 类似于集成学习（多个模型单独学习，推理时取多个模型的输出的平均值）
>
> - 学习时：随机删除神经元->每一次都让不同的模型进行学习
> - 推理时：对神经元的输出乘以删除比例->取得模型的平均值

![image-20240924144742889](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240924144742889.png)

```py
class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            # *：将元组解包，把3和4作为单独的参数传递给np.random.rand函数（生成一个形状为(3, 4)的随机矩阵）
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            # 每次正向传播，self.mask会以False的形式保存要删除的神经元
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask
```

### 高效寻找超参数

> 各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等

#### 什么是验证数据？

- 训练数据：参数（权重和偏置）的学习
- 验证数据：`超参数的性能评估`
- 测试数据：确认泛化能力

例：

对于minist数据集，为了获得验证数据，可以从训练数据中事先分割20%作为验证数据

#### 优化超参数

`观察可以使学习顺利进行的超参数范围来缩小值的范围。在缩小的范围重复相同的操作。在某个阶段选择一个最终的超参数的值`

**优化方法**：

1. 设定超参数范围【大致指定即可，如10^(-3)^~10^(3)^【也表述为：用对数尺度（log scale）指定】
2. 从设定的超参数范围随机采样
3. 使用采样到的超参数的值进行学习，通过验证数据评估经度（epoch设置要小：因为搞一次很浪费时间）
4. 重复2-3（100次等），根据经度结果缩小超参数范围

## 卷积神经网络

`CNN被用于图像识别、语音识别等场合，尤其是图像识别`

### CNN与全连接

全连接的劣势：

1. 没有保留图片的空间结构
2. 参数巨大

CNN的劣势：

1. 只考虑图上就近的特征（不如transformer有全局观）

### 核心思想

> 训练参数减少

- 局部连接【每一层和前一层的局部连接】
- 权重共享【每个输入数据块与同一个卷积核内积】

### 卷积层

`全连接层将输入转化为一维数据，无法利用形状相关的信息。`

> 输入特征图：卷积层的输入数据
>
> 输出特征图：卷积层的输出数据

#### 如何运算

`图像处理的卷积核运算（也称卷积核）`

**权重**

1. 各个位置输入数据的元素与卷积核的元素==对应位置相乘再求和==（乘积累加运算）
2. 重复进行直到做完

![image-20240925204151500](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925204151500.png)

**偏置**

![image-20240925204523849](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925204523849.png)

#### 卷积核设置

##### 卷积核大小

> 一般都是3个，越小越好，一般没有小于3的。如：3X3，5X5

##### 卷积核个数

> 问：卷积核数量如何设置？
>
> 答：使用多个小卷积核得到的效果 > 使用一个大卷积核【1.relu使用变多】
>
> 卷积核参数个数与图像大小无关 

|          | 通道数  | 高度          | 长度         | 表示规模  |
| -------- | ------- | ------------- | ------------ | --------- |
| 输入数据 | channel | height        | width        | (C,H,W)   |
| 卷积核   | channel | filter height | filter width | (C,FH,FW) |

- FN：卷积核数量

![image-20240925211339492](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925211339492.png)

##### 填充和步幅

> - 填充（padding）：在输入数据周围填充固定数据。**与输出大小正相关**
> - 步幅（stride）：应用卷积核的位置间隔。**越小越好，与输出大小负相关**
>
> 使用填充可以在保持空间大小不变的情况下将数据传输给下层

**填充**

`为了调整输出大小，可以将输入数据的周围填入固定数据（比如0等）`

![image-20240925205042409](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925205042409.png)

**步幅**

![image-20240925210316134](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925210316134.png)

##### ==输出shape计算公式==

- 输出大小为(OH,OW)；
- 输入大小为(H,W)；卷积核大小为(FH,FW)；填充：P；步幅：S

$$
\begin{aligned}&OH=\frac{H+2P-FH}{S}+1\\&OW=\frac{W+2P-FW}{S}+1\end{aligned}
$$
==注意：==所设定的值必须使得这两个公式可以除尽（有的框架当无法除尽会进行四舍五入/向下取整不报错，但需要采取措施进行报错）

##### 通道数

> 通道数设置：输入数据和卷积核相同

`通道方向有多个特征图时，按通道进行输入数据和卷积核的卷积核运算，将结果相加得到输出`

![image-20240925210101774](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925210101774.png)

#### 卷积次数

> 卷积次数要多层。越深层的卷积，感受野越大，神经网络从局部到全局

#### 感受野

`输出特征图上某个元素受输入图像上影响的区域`

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/5f42b016089be4a9f705afd88ab822ea.png" alt="img" style="zoom: 80%;" />

假设输入大小都是$h*w*c$，并且都使用c个卷积核（得到c个特征图），可以计算其各自所需参数：
$$
\text{一个}7*7\text{卷积核所需参数}=C×(7×7×C)=49C^2\\
\text{三个}7*7\text{卷积核所需参数}=3×C×(3×3×C)=27C^2
$$
很明显，堆叠小的卷积核所需的参数更少一些，并且卷积过程越多，特征提取也会越细致，加入的非线性变换也随着增多，还不会增大权重参数个数，这就是VGG网络的基本出发点，用小的卷积核来完成体特征提取操作

#### 批处理

`各层间的传递数据保存为4维数据（batch_num,channel,height,width）`

![image-20240925211607463](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925211607463.png)

### 池化层

`使用数据压缩，在减小计算量的同时防止过拟合`

> - Max池化：计算目标区域的最大值
> - Average池化：计算目标区域的平均值（不常用）
>
> 一般来讲，池化的窗口大小会和步幅设定为相同值

池化层的特征：

- 没有要学习的参数【只是从目标区域取最大值（或平均值），不存在要学习的参数】
- 通道数不发生变化【按照通道减小数据】
- 对微小的位置变化具有健壮性【平移不变性、旋转不变性、尺度不变性】
  <img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925213812074.png" alt="image-20240925213812074" style="zoom:50%;" />

池化过程：

![image-20240925215841772](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925215841772.png)

### 代码实现

#### im2col函数

`图像 —> 矩阵`

> numpy中使用for循环访问元素会变慢，所以可以使用im2col函数

![image-20240925214602005](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240925214602005.png)

```py
def im2col(input_data, filter_h, filter_w, stride=1, pad=0):
    """
    Parameters
    ----------
    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据
    filter_h : 卷积核的高
    filter_w : 卷积核的长
    stride : 步幅
    pad : 填充

    Returns
    -------
    col : 2维数组
    """
    N, C, H, W = input_data.shape
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1

    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return col
```

#### 卷积层实现

```py
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
        # 中间数据（backward时使用）
        self.x = None   
        self.col = None
        self.col_W = None
        
        # 权重和偏置参数的梯度
        self.dW = None
        self.db = None

    def forward(self, x):
        """
        卷积核是(FN,C,FH,FW)的四维数组
        FN：卷积核数量
        C：通道数
        FH：高
        FW：宽
        """
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)
        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)  # 输入数据的展开
        col_W = self.W.reshape(FN, -1).T    # 卷积核的展开

        out = np.dot(col, col_W) + self.b
        # reshape指定-1：自动计算-1维度上的元素个数来使多维数组的元素个数前后一致
        # tranpose函数：更改多维数组轴的顺序
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        self.x = x
        self.col = col
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0,2,3,1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx
```

#### 池化层实现

```py
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
		# 展开输入数据
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
		# 求各行最大值
        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)	# np.max：指定axis参数，并参数的各个轴方向求最大值
        # 转换为合适的输出大小
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max

        return out

    def backward(self, dout):
        dout = dout.transpose(0, 2, 3, 1)
        
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        
        return dx
```

#### CNN实现

```py
class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
                 conv_param={'filter_num':30, 'filter_size':5,
                             'pad':0, 'stride':1},
                 hidden_size=100, output_size=10, weight_init_std=0.01):
        """
        input_dim--输入数据的维度：（通道，高，长）
        conv_param——卷积层的超参数（字典）
            filter_num——卷积核的数量
            filter_size——卷积核的大小
            stride——步幅
            pad——填充
        hidden_size——隐藏层（全连接）的神经元数量
        output_size——输出层（全连接）的神经元数量·weitght_int_std——初始化时权重的标准差
        """
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1
                            
        pool_output_size = int(filter_num * (conv_output_size/2) *
                               (conv_output_size/2))
        
        # # 权重参数初始化
        self.params = {}
        # 卷积层的权重、偏置
        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0],filter_size, filter_size)                    
        self.params['b1'] = np.zeros(filter_num)
        # 第一个全连接层的权重、偏置
        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size,hidden_size)                    
        self.params['b2'] = np.zeros(hidden_size)
        # 第二个全连接层的权重、偏置
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b3'] = np.zeros(output_size)

        # # 生成必要层
        # OrderedDict会保持元素的插入顺序
        # 如果迭代OrderedDict，元素按照被添加的顺序返回
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'],
                                        self.params['b1'],
                                        conv_param['stride'],
                                        conv_param['pad'])

        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'],
                                    self.params['b2'])

        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'],
                                    self.params['b3'])
        self.last_layer = SoftmaxWithLoss()

    def predict(self, x):
        # x：输入数据
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):   # 除了使用predict方法进行的forward处理之外，还会继续直到softmaxwithloss层
        # x：输入数据
        # t：监督标签
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def gradient(self, x, t):
        """
        “基于误差反向传播法求梯度”
        将正向传播和反向传播组装在一起
        把各个权重参数的梯度保存到grads字典中
        """
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 设定
        grads = {}
        grads['W1'] = self.layers['Conv1'].dW
        grads['b1'] = self.layers['Conv1'].db
        grads['W2'] = self.layers['Affine1'].dW
        grads['b2'] = self.layers['Affine1'].db
        grads['W3'] = self.layers['Affine2'].dW
        grads['b3'] = self.layers['Affine2'].db

        return grads
```

### 卷积核变化图

> 卷积核在做什么？观察边缘（颜色变化的分界线）和斑块（局部的块状区域）
>
> 随着层次加深，提取的信息越来越抽象，神经元从简单的形状向“高级”信息变化

卷积层权重示意图：

![image-20240926093615857](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240926093615857.png)

卷积层权重示意图：

![image-20240926093806417](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240926093806417.png)

### CNN架构设计

> 为什么卷积几次才池化一次？需要累积特征

#### LeNet

![image-20240926094851231](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/image-20240926094851231.png)

1. 激活函数：sigmoid
2. 池化层：subsampling

当前流行的CNN：

1. 激活函数：ReLU
2. 池化层：Max池化

#### AlexNet

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/dbcbcc632ea823bc331e7fae8d2790ff-1727403602440-3.png" alt="dbcbcc632ea823bc331e7fae8d2790ff" style="zoom:80%;" />

1. 激活函数：ReLU
2. 池化：Max池化
3. 使用局部正规化层【LRN（Local Response Normalization）】
4. 使用Dropout

#### Vgg

> 做完池化之后特征减少，需要马上补特征

vgg网络架构图：

<img src="./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/1233445.png" style="zoom: 50%;" />

#### Resnet

`残差神经网络：模型训练过程至少不比原来差`

网络层数越多误差越大？？？

![img](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/v2-dcf5688dad675cbe8fb8be243af5e1fd_r.jpg)

解决方案：

将前某层的结果做恒等映射到当前层

![img](./%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%a5%e9%97%a8.assets/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.webp)

## 函数分类

`机器学习问题：回归问题、分类问题`

感知机：

- 单层感知机（朴素感知机）使用阶跃函数作为激活函数
- 多层感知机使用平滑的激活函数，如sigmoid函数

回归问题：

|          | 隐藏层            | 输出层   |
| -------- | ----------------- | -------- |
| 激活函数 | sigmoid、恒等函数 | 恒等函数 |
| 损失函数 |                   | 均方误差 |

分类问题：

|          | 隐藏层                       | 输出层  |
| -------- | ---------------------------- | ------- |
| 激活函数 | sigmoid(二元)、softmax(多元) | softmax |
| 损失函数 |                              | 交叉熵  |

## 附录

| 符号      | latex   | 含义                 |
| --------- | ------- | -------------------- |
| $\oplus$  | \oplus  | 矩阵加               |
| $\ominus$ | \ominus | 矩阵减               |
| $\otimes$ | \otimes | 矩阵乘法             |
| $\odot$   | \odot   | 点积【对应元素相乘】 |
